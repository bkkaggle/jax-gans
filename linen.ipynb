{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from glob import glob\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import wandb\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "        self.paths_a = glob('./data/trainA/*.jpg')\n",
    "        self.paths_b = glob('./data/trainB/*.jpg')\n",
    "\n",
    "        self.imgs_a = np.zeros(\n",
    "            (len(self.paths_a), 32, 32, 3), dtype=np.float32)\n",
    "        for i, path in tqdm(enumerate(self.paths_a)):\n",
    "            img = np.asarray(Image.open(path)) / 255.0\n",
    "            img = cv2.resize(img, dsize=(\n",
    "                32, 32), interpolation=cv2.INTER_CUBIC).reshape(1, 32, 32, 3)\n",
    "            self.imgs_a[i] = img\n",
    "\n",
    "        self.imgs_b = np.zeros(\n",
    "            (len(self.paths_b), 32, 32, 3), dtype=np.float32)\n",
    "        for i, path in tqdm(enumerate(self.paths_b)):\n",
    "            img = np.asarray(Image.open(path)) / 255.0\n",
    "            img = cv2.resize(img, dsize=(\n",
    "                32, 32), interpolation=cv2.INTER_CUBIC).reshape(1, 32, 32, 3)\n",
    "            self.imgs_b[i] = img\n",
    "\n",
    "        self.transforms = transforms.Compose([\n",
    "            # transforms.ToTensor(),\n",
    "            # transforms.ToPILImage(),\n",
    "            # transforms.RandomHorizontalFlip(),\n",
    "            # transforms.ColorJitter(hue=0.15),\n",
    "            # transforms.RandomGrayscale(p=0.25),\n",
    "            # transforms.RandomRotation(35),\n",
    "            # transforms.RandomPerspective(distortion_scale=0.35),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths_a)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_a = self.imgs_a[index]\n",
    "        img_b = self.imgs_b[index]\n",
    "\n",
    "        img_a = self.transforms(img_a).numpy().transpose(1, 2, 0)\n",
    "        img_b = self.transforms(img_b).numpy().transpose(1, 2, 0)\n",
    "        # img_a = img_a.transpose(1, 2, 0)\n",
    "        # img_b = img_b.transpose(1, 2, 0)\n",
    "\n",
    "        return img_a, img_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shard(xs):\n",
    "    return jax.tree_map(\n",
    "        lambda x: x.reshape((jax.device_count(), -1) + x.shape[1:]), xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleEncoder(nn.Module):\n",
    "    training: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    training: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, z):\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    training: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, z):\n",
    "        x = nn.ConvTranspose(features=64*8, kernel_size=(4, 4),\n",
    "                             strides=(1, 1), padding='VALID', use_bias=False)(z)\n",
    "        x = nn.BatchNorm(\n",
    "            use_running_average=not self.training, momentum=0.9)(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "        x = nn.ConvTranspose(features=64*4, kernel_size=(4, 4),\n",
    "                             strides=(2, 2), padding='SAME', use_bias=False)(x)\n",
    "        x = nn.BatchNorm(\n",
    "            use_running_average=not self.training, momentum=0.9)(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "        x = nn.ConvTranspose(features=64*2, kernel_size=(4, 4),\n",
    "                             strides=(2, 2), padding='SAME', use_bias=False)(x)\n",
    "        x = nn.BatchNorm(\n",
    "            use_running_average=not self.training, momentum=0.9)(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "        x = nn.ConvTranspose(features=64, kernel_size=(\n",
    "            4, 4), strides=(2, 2), padding='SAME', use_bias=False)(x)\n",
    "        x = nn.BatchNorm(\n",
    "            use_running_average=not self.training, momentum=0.9)(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "        x = nn.ConvTranspose(features=3, kernel_size=(\n",
    "            4, 4), strides=(1, 1), padding='SAME', use_bias=False)(x)\n",
    "        return jnp.tanh(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    training: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Conv(features=64, kernel_size=(\n",
    "            4, 4), strides=(2, 2), padding='SAME', use_bias=False)(x)\n",
    "        x = nn.leaky_relu(x, negative_slope=0.2)\n",
    "\n",
    "        x = nn.Conv(features=64*2, kernel_size=(4, 4),\n",
    "                    strides=(2, 2), padding='SAME', use_bias=False)(x)\n",
    "        x = nn.BatchNorm(\n",
    "            use_running_average=not self.training, momentum=0.9)(x)\n",
    "        x = nn.leaky_relu(x, negative_slope=0.2)\n",
    "\n",
    "        x = nn.Conv(features=64*4, kernel_size=(4, 4),\n",
    "                    strides=(2, 2), padding='SAME', use_bias=False)(x)\n",
    "        x = nn.BatchNorm(\n",
    "            use_running_average=not self.training, momentum=0.9)(x)\n",
    "        x = nn.leaky_relu(x, negative_slope=0.2)\n",
    "\n",
    "        x = nn.Conv(features=64*8, kernel_size=(4, 4),\n",
    "                    strides=(2, 2), padding='SAME', use_bias=False)(x)\n",
    "        x = nn.BatchNorm(\n",
    "            use_running_average=not self.training, momentum=0.9)(x)\n",
    "        x = nn.leaky_relu(x, negative_slope=0.2)\n",
    "\n",
    "        x = nn.Conv(features=1, kernel_size=(\n",
    "            1, 1), strides=(4, 4), padding='VALID', use_bias=False)(x)\n",
    "        x = jnp.reshape(x, [x.shape[0], -1])\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "@jax.vmap\n",
    "def bce_logits_loss(logit, label):\n",
    "    return jnp.maximum(logit, 0) - logit * label + jnp.log(1 + jnp.exp(-jnp.abs(logit)))\n",
    "\n",
    "\n",
    "def loss_g(params_g, params_d, batch, rng, variables_g, variables_d):\n",
    "    z = jax.random.normal(rng, shape=(batch.shape[0], 1, 1, 100))\n",
    "\n",
    "    fake_batch, variables_g = Generator(training=True).apply(\n",
    "        {'params': params_g, 'batch_stats': variables_g['batch_stats']}, z, mutable=['batch_stats'])\n",
    "\n",
    "    fake_logits, variables_d = Discriminator(training=True).apply(\n",
    "        {'params': params_d, 'batch_stats': variables_d['batch_stats']}, fake_batch, mutable=['batch_stats'])\n",
    "\n",
    "    real_labels = jnp.ones((batch.shape[0],), dtype=jnp.int32)\n",
    "    return jnp.mean(bce_logits_loss(fake_logits, real_labels)), (variables_g, variables_d)\n",
    "\n",
    "\n",
    "def loss_d(params_d, params_g, batch, rng, variables_g, variables_d):\n",
    "    z = jax.random.normal(rng, shape=(batch.shape[0], 1, 1, 100))\n",
    "\n",
    "    fake_batch, variables_g = Generator(training=True).apply(\n",
    "        {'params': params_g, 'batch_stats': variables_g['batch_stats']}, z, mutable=['batch_stats'])\n",
    "\n",
    "    real_logits, variables_d = Discriminator(training=True).apply(\n",
    "        {'params': params_d, 'batch_stats': variables_d['batch_stats']}, batch, mutable=['batch_stats'])\n",
    "    fake_logits, variables_d = Discriminator(training=True).apply(\n",
    "        {'params': params_d, 'batch_stats': variables_d['batch_stats']}, fake_batch, mutable=['batch_stats'])\n",
    "\n",
    "    real_labels = jnp.ones((batch.shape[0],), dtype=jnp.int32)\n",
    "    real_loss = bce_logits_loss(real_logits, real_labels)\n",
    "\n",
    "    fake_labels = jnp.zeros((batch.shape[0],), dtype=jnp.int32)\n",
    "    fake_loss = bce_logits_loss(fake_logits, fake_labels)\n",
    "\n",
    "    return jnp.mean(real_loss + fake_loss), (variables_g, variables_d)\n",
    "\n",
    "\n",
    "@partial(jax.pmap, axis_name='batch')\n",
    "def train_step(rng, variables_g, variables_d, optimizer_g, optimizer_d, batch):\n",
    "    rng, rng_g, rng_d = jax.random.split(rng, 3)\n",
    "\n",
    "    (g_loss, (variables_g, variables_d)), grad_g = jax.value_and_grad(loss_g, has_aux=True)(\n",
    "        optimizer_g.target, optimizer_d.target, batch, rng_g, variables_g, variables_d)\n",
    "    g_loss = jax.lax.pmean(g_loss, axis_name='batch')\n",
    "    grad_g = jax.lax.pmean(grad_g, axis_name='batch')\n",
    "\n",
    "    optimizer_g = optimizer_g.apply_gradient(grad_g)\n",
    "\n",
    "    (d_loss, (variables_g, variables_d)), grad_d = jax.value_and_grad(loss_d, has_aux=True)(\n",
    "        optimizer_d.target, optimizer_g.target, batch, rng_d, variables_g, variables_d)\n",
    "\n",
    "    d_loss = jax.lax.pmean(d_loss, axis_name='batch')\n",
    "    grad_d = jax.lax.pmean(grad_d, axis_name='batch')\n",
    "\n",
    "    optimizer_d = optimizer_d.apply_gradient(grad_d)\n",
    "\n",
    "    return rng, variables_g, variables_d, optimizer_g, optimizer_d, d_loss, g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    wandb.init(project='flax-dcgan-selfie')\n",
    "\n",
    "    dataset = GANDataset({})\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
    "\n",
    "    rng = jax.random.PRNGKey(42)\n",
    "    rng, rng_g, rng_d = jax.random.split(rng, 3)\n",
    "\n",
    "    init_batch_g = jnp.ones((1, 1, 1, 100), jnp.float32)\n",
    "    variables_g = Generator(training=True).init(rng_g, init_batch_g)\n",
    "\n",
    "    init_batch_d = jnp.ones((1, 32, 32, 3), jnp.float32)\n",
    "    variables_d = Discriminator(training=True).init(rng_d, init_batch_d)\n",
    "\n",
    "    optimizer_g = flax.optim.Adam(\n",
    "        learning_rate=1e-4, beta1=0.5, beta2=0.9).create(variables_g[\"params\"])\n",
    "    optimizer_g = flax.jax_utils.replicate(optimizer_g)\n",
    "\n",
    "    optimizer_d = flax.optim.Adam(\n",
    "        learning_rate=1e-4, beta1=0.5, beta2=0.9).create(variables_d[\"params\"])\n",
    "    optimizer_d = flax.jax_utils.replicate(optimizer_d)\n",
    "\n",
    "    variables_g = flax.jax_utils.replicate(variables_g)\n",
    "    variables_d = flax.jax_utils.replicate(variables_d)\n",
    "\n",
    "    rngs = jax.random.split(rng, num=jax.local_device_count())\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(100):\n",
    "        for i, (img_a, img_b) in tqdm(enumerate(train_dataloader)):\n",
    "            img_a = shard(img_a.numpy())\n",
    "            img_b = shard(img_b.numpy())\n",
    "\n",
    "            rngs, variables_g, variables_d, optimizer_g, optimizer_d, d_loss, g_loss = train_step(\n",
    "                rngs, variables_g, variables_d, optimizer_g, optimizer_d, img_a)\n",
    "\n",
    "            if global_step % 10 == 0:\n",
    "                to_log = {'g_loss': float(jnp.mean(g_loss)),\n",
    "                          'd_loss': float(jnp.mean(d_loss))}\n",
    "                if global_step % 100 == 0:\n",
    "                    rng, rng_sample = jax.random.split(rng)\n",
    "                    z = jax.random.normal(rng_sample, shape=(1, 1, 1, 100))\n",
    "\n",
    "                    temp_params_g = flax.jax_utils.unreplicate(\n",
    "                        optimizer_g.target)\n",
    "                    temp_variables_g = flax.jax_utils.unreplicate(variables_g)\n",
    "\n",
    "                    samples = Generator(training=False).apply(\n",
    "                        {'params': temp_params_g, 'batch_stats': temp_variables_g['batch_stats']}, z, mutable=False)\n",
    "\n",
    "                    img = jnp.reshape((samples + 1) / 2, [32, 32, 3])\n",
    "                    to_log['img'] = wandb.Image(np.array(img))\n",
    "                wandb.log(to_log)\n",
    "\n",
    "            global_step += 1\n",
    "@dataclass\n",
    "class Data:\n",
    "    batch_size: int\n",
    "    num_workers: int\n",
    "\n",
    "\n",
    "main(Data(256, 4))"
   ]
  }
 ]
}