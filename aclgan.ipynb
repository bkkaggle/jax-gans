{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from glob import glob\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import wandb\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from glob import glob\n",
    "from functools import partial\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import wandb\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class GANDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "        self.paths_a = glob('./data/trainA/*.jpg')\n",
    "        self.paths_b = glob('./data/trainB/*.jpg')\n",
    "\n",
    "        self.imgs_a = np.zeros(\n",
    "            (len(self.paths_a), 64, 64, 3), dtype=np.float32)\n",
    "        for i, path in tqdm(enumerate(self.paths_a)):\n",
    "            img = np.asarray(Image.open(path)) / 255.0\n",
    "            img = cv2.resize(img, dsize=(\n",
    "                64, 64), interpolation=cv2.INTER_CUBIC).reshape(1, 64, 64, 3)\n",
    "            self.imgs_a[i] = img\n",
    "\n",
    "        self.imgs_b = np.zeros(\n",
    "            (len(self.paths_b), 64, 64, 3), dtype=np.float32)\n",
    "        for i, path in tqdm(enumerate(self.paths_b)):\n",
    "            img = np.asarray(Image.open(path)) / 255.0\n",
    "            img = cv2.resize(img, dsize=(\n",
    "                64, 64), interpolation=cv2.INTER_CUBIC).reshape(1, 64, 64, 3)\n",
    "            self.imgs_b[i] = img\n",
    "\n",
    "        self.transforms = transforms.Compose([\n",
    "            # transforms.ToTensor(),\n",
    "            # transforms.ToPILImage(),\n",
    "            # transforms.RandomHorizontalFlip(),\n",
    "            # transforms.ColorJitter(hue=0.15),\n",
    "            # transforms.RandomGrayscale(p=0.25),\n",
    "            # transforms.RandomRotation(35),\n",
    "            # transforms.RandomPerspective(distortion_scale=0.35),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths_a)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_a = self.imgs_a[index]\n",
    "        img_b = self.imgs_b[index]\n",
    "\n",
    "        img_a = self.transforms(img_a).numpy().transpose(1, 2, 0)\n",
    "        img_b = self.transforms(img_b).numpy().transpose(1, 2, 0)\n",
    "        # img_a = img_a.transpose(1, 2, 0)\n",
    "        # img_b = img_b.transpose(1, 2, 0)\n",
    "\n",
    "        return img_a, img_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shard(xs):\n",
    "    return jax.tree_map(\n",
    "        lambda x: x.reshape((jax.device_count(), -1) + x.shape[1:]), xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IN(nn.Module):\n",
    "    def __call__(self, x):\n",
    "        # assuming an image in the format HxWxC\n",
    "        mu = jnp.mean(x, axis=(1, 2), keepdims=True)\n",
    "        sigma_2 = jnp.var(x, axis=(1, 2), keepdims=True)\n",
    "\n",
    "        return (x - mu) / jnp.sqrt(sigma_2 + 1e-5)\n",
    "\n",
    "\n",
    "class AdaIN(nn.Module):\n",
    "    @nn.compact\n",
    "    # assuming an image in the format HxWxC\n",
    "    def __call__(self, content, style_mean, style_var):\n",
    "        content_mu = jnp.mean(content, axis=(1, 2), keepdims=True)\n",
    "        content_sigma_2 = jnp.var(content, axis=(1, 2), keepdims=True)\n",
    "\n",
    "        normalized_content = (content - content_mu) / \\\n",
    "            jnp.sqrt(content_sigma_2 + 1e-5)\n",
    "\n",
    "        return normalized_content * \\\n",
    "            jnp.sqrt(style_var + 1e-5) + style_mean\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    padding: int\n",
    "    features: int\n",
    "    kernel_size: int\n",
    "    strides: int\n",
    "    norm: str = \"none\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, means=None, variances=None):\n",
    "        x = jnp.pad(x, [(0, 0), (self.padding, self.padding),\n",
    "                        (self.padding, self.padding), (0, 0)], mode='constant')\n",
    "\n",
    "        if self.norm == \"IN\":\n",
    "            x = IN()(x)\n",
    "        elif self.norm == 'AdaIN' and means is not None and variances is not None:\n",
    "            x = AdaIN()(x, means, variances)\n",
    "\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features=self.features, kernel_size=(self.kernel_size, self.kernel_size),\n",
    "                    strides=(self.strides, self.strides), padding='VALID')(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    padding: int\n",
    "    features: int\n",
    "    kernel_size: int\n",
    "    strides: int\n",
    "    norm: str = \"none\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, means=None, variances=None):\n",
    "        residual_branch = x\n",
    "\n",
    "        x = Block(self.padding, self.features,\n",
    "                  self.kernel_size, self.strides, self.norm)(x, means, variances)\n",
    "        x = Block(self.padding, self.features,\n",
    "                  self.kernel_size, self.strides, \"none\")(x, means, variances)\n",
    "\n",
    "        return x + residual_branch\n",
    "\n",
    "\n",
    "class StyleEncoder(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = Block(padding=3, features=64, kernel_size=7, strides=1)(x)\n",
    "\n",
    "        x = Block(padding=1, features=64*2, kernel_size=4, strides=2)(x)\n",
    "        x = Block(padding=1, features=64*2*2,\n",
    "                  kernel_size=4, strides=2)(x)\n",
    "\n",
    "        x = Block(padding=1, features=64*2*2,\n",
    "                  kernel_size=4, strides=2)(x)\n",
    "        x = Block(padding=1, features=64*2*2,\n",
    "                  kernel_size=4, strides=2)(x)\n",
    "\n",
    "        x = jnp.mean(x, axis=(1, 2), keepdims=True)\n",
    "        x = nn.Conv(features=8, kernel_size=(1, 1),\n",
    "                    strides=(1, 1), padding='VALID')(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ContentEncoder(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "\n",
    "        x = Block(padding=3, features=64, kernel_size=7,\n",
    "                  strides=1, norm='IN')(x)\n",
    "\n",
    "        x = Block(padding=1, features=64*2,\n",
    "                  kernel_size=4, strides=2, norm='IN')(x)\n",
    "        x = Block(padding=1, features=64*2*2,\n",
    "                  kernel_size=4, strides=2, norm='IN')(x)\n",
    "\n",
    "        x = ResBlock(\n",
    "            padding=1, features=64*2*2, kernel_size=3, strides=1, norm='IN')(x)\n",
    "        x = ResBlock(\n",
    "            padding=1, features=64*2*2, kernel_size=3, strides=1, norm='IN')(x)\n",
    "        x = ResBlock(\n",
    "            padding=1, features=64*2*2, kernel_size=3, strides=1, norm='IN')(x)\n",
    "        x = ResBlock(\n",
    "            padding=1, features=64*2*2, kernel_size=3, strides=1, norm='IN')(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=256, use_bias=True)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=256, use_bias=True)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=256, use_bias=True)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=256, use_bias=True)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=4*256*2, use_bias=True)(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x, AdaIN_params):\n",
    "\n",
    "        x = ResBlock(padding=1, features=64*2*2, kernel_size=3,\n",
    "                     strides=1, norm='AdaIN')(x, AdaIN_params[:, :, :, :256], AdaIN_params[:, :, :, 256:256*2])\n",
    "        x = ResBlock(padding=1, features=64*2*2, kernel_size=3,\n",
    "                     strides=1, norm='')(x, AdaIN_params[:, :, :, 256*2:256*2+256], AdaIN_params[:, :, :, 256*2+256:256*4])\n",
    "        x = ResBlock(padding=1, features=64*2*2, kernel_size=3,\n",
    "                     strides=1, norm='')(x, AdaIN_params[:, :, :, 256*4:256*4+256], AdaIN_params[:, :, :, 256*4+256:256*6])\n",
    "        x = ResBlock(padding=1, features=64*2*2, kernel_size=3,\n",
    "                     strides=1, norm='')(x, AdaIN_params[:, :, :, 256*6:256*6+256], AdaIN_params[:, :, :, 256*6+256:256*8])\n",
    "\n",
    "        x = jax.image.resize(\n",
    "            x, (x.shape[0], x.shape[1]*2, x.shape[2]*2, x.shape[3]), method=jax.image.ResizeMethod.NEAREST)\n",
    "        x = Block(padding=2, features=64*2, kernel_size=5,\n",
    "                  strides=1, norm='IN')(x)\n",
    "\n",
    "        x = jax.image.resize(\n",
    "            x, (x.shape[0], x.shape[1]*2, x.shape[2]*2, x.shape[3]), method=jax.image.ResizeMethod.NEAREST)\n",
    "        x = Block(padding=2, features=64, kernel_size=5,\n",
    "                  strides=1, norm='IN')(x)\n",
    "\n",
    "        x = Block(padding=3, features=3, kernel_size=7, strides=1)(x)\n",
    "        x = jnp.tanh(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class G_enc(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        content = ContentEncoder()(x)\n",
    "        style = StyleEncoder()(x)\n",
    "\n",
    "        return content, style\n",
    "\n",
    "\n",
    "class G_dec(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x, style):\n",
    "        AdaIN_params = MLP()(style)\n",
    "        out = Decoder()(x, AdaIN_params)\n",
    "        return out\n",
    "\n",
    "\n",
    "class D_part(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = jnp.pad(x, [(0, 0), (1, 1), (1, 1), (0, 0)], mode='constant')\n",
    "        x = nn.leaky_relu(x, negative_slope=0.2)\n",
    "        x = nn.Conv(features=64, kernel_size=(4, 4),\n",
    "                    strides=(2, 2), padding='VALID')(x)\n",
    "\n",
    "        x = jnp.pad(x, [(0, 0), (1, 1), (1, 1), (0, 0)], mode='constant')\n",
    "        x = nn.leaky_relu(x, negative_slope=0.2)\n",
    "        x = nn.Conv(features=64*2, kernel_size=(4, 4),\n",
    "                    strides=(2, 2), padding='VALID')(x)\n",
    "\n",
    "        x = jnp.pad(x, [(0, 0), (1, 1), (1, 1), (0, 0)], mode='constant')\n",
    "        x = nn.leaky_relu(x, negative_slope=0.2)\n",
    "        x = nn.Conv(features=64*2*2, kernel_size=(4, 4),\n",
    "                    strides=(2, 2), padding='VALID')(x)\n",
    "\n",
    "        x = jnp.pad(x, [(0, 0), (1, 1), (1, 1), (0, 0)], mode='constant')\n",
    "        x = nn.leaky_relu(x, negative_slope=0.2)\n",
    "        x = nn.Conv(features=64*2*2*2, kernel_size=(4, 4),\n",
    "                    strides=(2, 2), padding='VALID')(x)\n",
    "\n",
    "        x = nn.Conv(features=1, kernel_size=(1, 1),\n",
    "                    strides=(1, 1), padding=\"VALID\")(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class D(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        outputs = []\n",
    "\n",
    "        out = D_part()(x)\n",
    "        outputs.append(out)\n",
    "\n",
    "        x = nn.avg_pool(x, window_shape=(3, 3), strides=(\n",
    "            2, 2), padding=((1, 1), (1, 1)))\n",
    "        out = D_part()(x)\n",
    "        outputs.append(out)\n",
    "\n",
    "        x = nn.avg_pool(x, window_shape=(3, 3), strides=(\n",
    "            2, 2), padding=((1, 1), (1, 1)))\n",
    "        out = D_part()(x)\n",
    "        outputs.append(out)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_loss(logits, labels):\n",
    "    loss = 0\n",
    "    for i in range(3):\n",
    "        loss += jnp.mean((logits[i]-labels[i])**2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def l1_loss(logits, labels):\n",
    "    return jnp.mean(jnp.abs(logits - labels))\n",
    "\n",
    "\n",
    "def loss_g(g_params, d_params, x_s, x_t, rng):\n",
    "    rng, rng_1, rng_2, rng_3 = jax.random.split(rng, 4)\n",
    "\n",
    "    bs = x_s.shape[0]\n",
    "    z_1 = jax.random.normal(rng_1, shape=(bs, 1, 1, 8))\n",
    "    z_2 = jax.random.normal(rng_2, shape=(bs, 1, 1, 8))\n",
    "    z_3 = jax.random.normal(rng_3, shape=(bs, 1, 1, 8))\n",
    "\n",
    "    c_s, z_s = G_enc().apply({'params': g_params['g_s_enc']}, x_s)\n",
    "    fake_s = G_dec().apply({'params': g_params['g_s_dec']}, c_s, z_1)\n",
    "\n",
    "    c_t, z_t = G_enc().apply({'params': g_params['g_t_enc']}, x_s)\n",
    "    fake_t = G_dec().apply({'params': g_params['g_t_dec']}, c_t, z_2)\n",
    "\n",
    "    c_recon_s, z_recon_s = G_enc().apply(\n",
    "        {'params': g_params['g_s_enc']}, fake_t)\n",
    "    fake_recon_s = G_dec().apply(\n",
    "        {'params': g_params['g_s_dec']}, c_recon_s, z_3)\n",
    "\n",
    "    fake_idt_s = G_dec().apply(\n",
    "        {'params': g_params['g_s_dec']}, c_s, z_s)\n",
    "    fake_idt_t = G_dec().apply(\n",
    "        {'params': g_params['g_t_dec']}, c_t, z_t)\n",
    "\n",
    "    fake_logits_s = D().apply({'params': d_params['d_s']}, fake_s)\n",
    "    fake_logits_t = D().apply({'params': d_params['d_t']}, fake_t)\n",
    "\n",
    "    fake_logits_recon_s = D().apply(\n",
    "        {'params': d_params['d_s']}, fake_recon_s)\n",
    "\n",
    "    s_fake_s_logits = D().apply(\n",
    "        {'params': d_params['d_hat']}, jnp.concatenate([x_s, fake_s], axis=3))\n",
    "    s_fake_recon_s_logits = D().apply(\n",
    "        {'params': d_params['d_hat']}, jnp.concatenate([x_s, fake_recon_s], axis=3))\n",
    "\n",
    "    real_adv_labels = [jnp.ones_like(fake_logits_s[0]), jnp.ones_like(\n",
    "        fake_logits_s[1]), jnp.ones_like(fake_logits_s[2])]\n",
    "    real_acl_labels = [jnp.ones_like(s_fake_s_logits[0]), jnp.ones_like(\n",
    "        s_fake_s_logits[1]), jnp.ones_like(s_fake_s_logits[2])]\n",
    "    fake_acl_labels = [jnp.zeros_like(s_fake_s_logits[0]), jnp.zeros_like(\n",
    "        s_fake_s_logits[1]), jnp.zeros_like(s_fake_s_logits[2])]\n",
    "\n",
    "    loss_adv = l2_loss(fake_logits_t, real_adv_labels) + (l2_loss(fake_logits_s,\n",
    "                                                                  real_adv_labels) + l2_loss(fake_logits_recon_s, real_adv_labels)) / 2\n",
    "    loss_acl = l2_loss(s_fake_s_logits, real_acl_labels) + \\\n",
    "        l2_loss(s_fake_recon_s_logits, fake_acl_labels)\n",
    "    loss_idt = l1_loss(x_s, fake_idt_s) + l1_loss(x_t, fake_idt_t)\n",
    "\n",
    "    loss = loss_adv + 0.5 * loss_acl + loss_idt\n",
    "\n",
    "    return loss, {'adv': loss_adv, 'acl': loss_acl, 'idt': loss_idt}\n",
    "\n",
    "\n",
    "def loss_d(d_params, g_params, x_s, x_t, rng):\n",
    "    rng, rng_1, rng_2, rng_3 = jax.random.split(rng, 4)\n",
    "\n",
    "    bs = x_s.shape[0]\n",
    "    z_1 = jax.random.normal(rng_1, shape=(bs, 1, 1, 8))\n",
    "    z_2 = jax.random.normal(rng_2, shape=(bs, 1, 1, 8))\n",
    "    z_3 = jax.random.normal(rng_3, shape=(bs, 1, 1, 8))\n",
    "\n",
    "    c_s, z_s = G_enc().apply({'params': g_params['g_s_enc']}, x_s)\n",
    "    fake_s = G_dec().apply({'params': g_params['g_s_dec']}, c_s, z_1)\n",
    "\n",
    "    c_t, z_t = G_enc().apply({'params': g_params['g_t_enc']}, x_s)\n",
    "    fake_t = G_dec().apply({'params': g_params['g_t_dec']}, c_t, z_2)\n",
    "\n",
    "    c_recon_s, z_recon_s = G_enc().apply(\n",
    "        {'params': g_params['g_s_enc']}, fake_t)\n",
    "    fake_recon_s = G_dec().apply(\n",
    "        {'params': g_params['g_s_dec']}, c_recon_s, z_3)\n",
    "\n",
    "    real_logits_s = D().apply({'params': d_params['d_s']}, x_s)\n",
    "    fake_logits_s = D().apply({'params': d_params['d_s']}, fake_s)\n",
    "\n",
    "    real_logits_t = D().apply({'params': d_params['d_t']}, x_t)\n",
    "    fake_logits_t = D().apply({'params': d_params['d_t']}, fake_t)\n",
    "\n",
    "    real_logits_recon_s = D().apply({'params': d_params['d_s']}, x_s)\n",
    "    fake_logits_recon_s = D().apply({'params': d_params['d_s']}, fake_recon_s)\n",
    "\n",
    "    s_fake_s_logits = D().apply(\n",
    "        {'params': d_params['d_hat']}, jnp.concatenate([x_s, fake_s], axis=3))\n",
    "    s_fake_recon_s_logits = D().apply(\n",
    "        {'params': d_params['d_hat']}, jnp.concatenate([x_s, fake_recon_s], axis=3))\n",
    "\n",
    "    real_adv_labels = [jnp.ones_like(fake_logits_s[0]), jnp.ones_like(\n",
    "        fake_logits_s[1]), jnp.ones_like(fake_logits_s[2])]\n",
    "    fake_adv_labels = [jnp.zeros_like(fake_logits_s[0]), jnp.zeros_like(\n",
    "        fake_logits_s[1]), jnp.zeros_like(fake_logits_s[2])]\n",
    "    real_acl_labels = [jnp.ones_like(s_fake_s_logits[0]), jnp.ones_like(\n",
    "        s_fake_s_logits[1]), jnp.ones_like(s_fake_s_logits[2])]\n",
    "    fake_acl_labels = [jnp.zeros_like(s_fake_s_logits[0]), jnp.zeros_like(\n",
    "        s_fake_s_logits[1]), jnp.zeros_like(s_fake_s_logits[2])]\n",
    "\n",
    "    loss_adv = l2_loss(fake_logits_t, fake_adv_labels) + l2_loss(real_logits_t, real_adv_labels) + \\\n",
    "        (l2_loss(fake_logits_s, fake_adv_labels) + l2_loss(real_logits_s, real_adv_labels) +\n",
    "         l2_loss(fake_logits_recon_s, fake_adv_labels) + l2_loss(real_logits_recon_s, real_adv_labels)) / 2\n",
    "    loss_acl = l2_loss(s_fake_s_logits, fake_acl_labels) + \\\n",
    "        l2_loss(s_fake_recon_s_logits, real_acl_labels)\n",
    "\n",
    "    loss = loss_adv + 0.5 * loss_acl\n",
    "\n",
    "    return loss, {'adv': loss_adv, 'acl': loss_acl}\n",
    "\n",
    "\n",
    "@partial(jax.pmap, axis_name='batch')\n",
    "def g_train(optimizer_g, optimizer_d, x_s, x_t, rng):\n",
    "    rng, rng_g = jax.random.split(rng)\n",
    "\n",
    "    (g_loss, g_losses), g_grad = jax.value_and_grad(loss_g, has_aux=True)(\n",
    "        optimizer_g.target, optimizer_d.target, x_s, x_t, rng_g)\n",
    "\n",
    "    g_loss = jax.lax.pmean(g_loss, axis_name='batch')\n",
    "    g_grad = jax.lax.pmean(g_grad, axis_name='batch')\n",
    "\n",
    "    optimizer_g = optimizer_g.apply_gradient(g_grad)\n",
    "\n",
    "    return rng, optimizer_g, optimizer_d, g_losses\n",
    "\n",
    "\n",
    "@partial(jax.pmap, axis_name='batch')\n",
    "def d_train(optimizer_g, optimizer_d, x_s, x_t, rng):\n",
    "    rng, rng_d = jax.random.split(rng)\n",
    "\n",
    "    (d_loss, d_losses), d_grad = jax.value_and_grad(loss_d, has_aux=True)(\n",
    "        optimizer_d.target, optimizer_g.target, x_s, x_t, rng_d)\n",
    "    d_loss = jax.lax.pmean(d_loss, axis_name='batch')\n",
    "    d_grad = jax.lax.pmean(d_grad, axis_name='batch')\n",
    "\n",
    "    optimizer_d = optimizer_d.apply_gradient(d_grad)\n",
    "\n",
    "    return rng, optimizer_g, optimizer_d, d_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "3400it [00:07, 467.81it/s]\n",
      "3400it [00:16, 207.44it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = GANDataset({})\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=8, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(42)\n",
    "rng, rng_g, rng_d = jax.random.split(rng, 3)\n",
    "\n",
    "init_x = jnp.ones((1, 64, 64, 3), jnp.float32)\n",
    "init_encoded = jnp.ones((1, 16, 16, 256), jnp.float32)\n",
    "init_z = jnp.zeros((1, 1, 1, 8), jnp.float32)\n",
    "init_x_acl = jnp.ones((1, 64, 64, 6), jnp.float32)\n",
    "\n",
    "variables_g_s_enc = G_enc().init(rng_g, init_x)\n",
    "variables_g_s_dec = G_dec().init(rng_g, init_encoded, init_z)\n",
    "\n",
    "variables_g_t_enc = G_enc().init(rng_g, init_x)\n",
    "variables_g_t_dec = G_dec().init(rng_g, init_encoded, init_z)\n",
    "\n",
    "variables_d_s = D().init(rng_d, init_x)\n",
    "variables_d_t = D().init(rng_d, init_x)\n",
    "variables_d_hat = D().init(rng_d, init_x_acl)\n",
    "\n",
    "g_params = {\n",
    "    'g_s_enc': variables_g_s_enc['params'],\n",
    "    'g_s_dec': variables_g_s_dec['params'],\n",
    "    'g_t_enc': variables_g_t_enc['params'],\n",
    "    'g_t_dec': variables_g_t_dec['params'],\n",
    "}\n",
    "\n",
    "d_params = {\n",
    "    'd_s': variables_d_s['params'],\n",
    "    'd_t': variables_d_t['params'],\n",
    "    'd_hat': variables_d_hat['params'],\n",
    "}\n",
    "\n",
    "optimizer_g = flax.optim.Adam(\n",
    "    learning_rate=1e-4, beta1=0.5, beta2=0.9).create(g_params)\n",
    "optimizer_g = flax.jax_utils.replicate(optimizer_g)\n",
    "\n",
    "optimizer_d = flax.optim.Adam(\n",
    "    learning_rate=1e-4, beta1=0.5, beta2=0.9).create(d_params)\n",
    "optimizer_d = flax.jax_utils.replicate(optimizer_d)\n",
    "\n",
    "rngs = jax.random.split(rng, num=jax.local_device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_params_sample = flax.jax_utils.unreplicate(optimizer_g.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = jax.tree_util.tree_leaves(g_params_sample)\n",
    "keys = []\n",
    "def traverse(prefix, tree):\n",
    "    if type(tree) is not jax.interpreters.xla._DeviceArray:\n",
    "        for child in list(tree.keys()):\n",
    "            traverse(f\"{prefix}/{child}\", tree[child])\n",
    "    else:\n",
    "        keys.append(prefix)\n",
    "traverse(\"parameters\", g_params_sample)\n",
    "to_log = {}\n",
    "for (key, value) in zip(keys, params):\n",
    "    to_log[key] = wandb.Histogram(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-259-e383685ee685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mx_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mx_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.1_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.1_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.1_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_spawn_posix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mForkServerProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.1_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.1_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.1_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mfds_to_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "for epoch in range(100):\n",
    "    for i, (x_s, x_t) in tqdm(enumerate(train_dataloader)):\n",
    "        x_s = shard(x_s.numpy())\n",
    "        x_t = shard(x_t.numpy())\n",
    "\n",
    "        if global_step % 1 == 0:\n",
    "            rngs, optimizer_g, optimizer_d, d_losses = d_train(\n",
    "                optimizer_g, optimizer_d, x_s, x_t, rngs)\n",
    "\n",
    "        if global_step % 2 == 0:\n",
    "            rngs, optimizer_g, optimizer_d, g_losses = g_train(\n",
    "                optimizer_g, optimizer_d, x_s, x_t, rngs)\n",
    "\n",
    "        if global_step % 10 == 0:\n",
    "            g_losses = flax.jax_utils.unreplicate(g_losses)\n",
    "            d_losses = flax.jax_utils.unreplicate(d_losses)\n",
    "\n",
    "            to_log = {'g_loss_adv': float(jnp.mean(g_losses['adv'])),\n",
    "                        'g_loss_acl': float(jnp.mean(g_losses['acl'])),\n",
    "                        'g_loss_idt': float(jnp.mean(g_losses['idt'])),\n",
    "                        'd_loss_adv': float(jnp.mean(d_losses['adv'])),\n",
    "                        'd_loss_acl': float(jnp.mean(d_losses['adv'])), }\n",
    "\n",
    "            if global_step % 10 == 0:\n",
    "                rng, rng_s, rng_t = jax.random.split(rng, num=3)\n",
    "                z_s = jax.random.normal(rng_s, shape=(1, 1, 1, 8))\n",
    "                z_t = jax.random.normal(rng_t, shape=(1, 1, 1, 8))\n",
    "                x_s = flax.jax_utils.unreplicate(\n",
    "                    x_s)[0].reshape(1, 64, 64, 3)\n",
    "                x_t = flax.jax_utils.unreplicate(\n",
    "                    x_t)[0].reshape(1, 64, 64, 3)\n",
    "\n",
    "                g_params_sample = flax.jax_utils.unreplicate(\n",
    "                    optimizer_g.target)\n",
    "\n",
    "                c_s, _ = G_enc().apply(\n",
    "                    {'params': g_params_sample['g_s_enc']}, x_t)\n",
    "                sample_s = G_dec().apply(\n",
    "                    {'params': g_params_sample['g_s_dec']}, c_s, z_s)\n",
    "\n",
    "                c_t, _ = G_enc().apply(\n",
    "                    {'params': g_params_sample['g_t_enc']}, x_s)\n",
    "                sample_t = G_dec().apply(\n",
    "                    {'params': g_params_sample['g_t_dec']}, c_t, z_t)\n",
    "\n",
    "                x_s = jnp.reshape((x_s + 1) / 2, [64, 64, 3])\n",
    "                x_t = jnp.reshape((x_t + 1) / 2, [64, 64, 3])\n",
    "\n",
    "                sample_s = jnp.reshape((sample_s + 1) / 2, [64, 64, 3])\n",
    "                sample_t = jnp.reshape((sample_t + 1) / 2, [64, 64, 3])\n",
    "\n",
    "                to_log['x_s'] = wandb.Image(np.array(x_s))\n",
    "                to_log['x_t'] = wandb.Image(np.array(x_t))\n",
    "\n",
    "                to_log['sample_s'] = wandb.Image(np.array(sample_s))\n",
    "                to_log['sample_t'] = wandb.Image(np.array(sample_t))\n",
    "\n",
    "            wandb.log(to_log)\n",
    "\n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}